# Proyecto de Visi√≥n por Computador: Extracci√≥n y Clasificaci√≥n de Keyframes de Videos de YouTube

## üìã Descripci√≥n del Proyecto

Este proyecto implementa un pipeline completo de procesamiento de videos de YouTube para la extracci√≥n autom√°tica de keyframes (frames clave) y su posterior clasificaci√≥n en categor√≠as espec√≠ficas. El sistema est√° dise√±ado para procesar videos de canales de trading y finanzas, extrayendo informaci√≥n visual relevante mediante t√©cnicas de visi√≥n por computador y aprendizaje autom√°tico.

El proyecto sigue la metodolog√≠a **CRISP-DM** (Cross-Industry Standard Process for Data Mining) para garantizar un desarrollo estructurado y reproducible.

---

## üéØ Metodolog√≠a CRISP-DM

### 1. Comprensi√≥n del Negocio

#### Problema a Resolver
Los videos de YouTube contienen una gran cantidad de informaci√≥n visual que puede ser dif√≠cil de procesar y analizar manualmente. En el contexto de canales de trading y finanzas, es crucial identificar y extraer los frames m√°s representativos que contengan informaci√≥n relevante (gr√°ficos, tablas, texto, personas, etc.) para:

- **Automatizar el an√°lisis de contenido**: Reducir el tiempo necesario para revisar videos completos
- **Extracci√≥n de informaci√≥n estructurada**: Identificar y categorizar elementos visuales clave
- **Generaci√≥n de res√∫menes visuales**: Crear representaciones compactas del contenido de videos
- **An√°lisis de tendencias**: Identificar patrones visuales en m√∫ltiples videos

#### Objetivos del Proyecto
1. Extraer autom√°ticamente keyframes representativos de videos de YouTube
2. Clasificar los frames en categor√≠as espec√≠ficas (gr√°ficos, tablas, texto, personas, etc.)
3. Generar descripciones textuales estructuradas de los frames usando modelos de lenguaje
4. Proporcionar una interfaz interactiva para el procesamiento de videos

#### Criterios de √âxito
- Reducci√≥n significativa del n√∫mero de frames a procesar (m√°s del 80%)
- Precisi√≥n de clasificaci√≥n superior al 90%
- Extracci√≥n de informaci√≥n financiera relevante de forma estructurada
- Pipeline funcional y reproducible

---

### 2. Comprensi√≥n de los Datos

#### Fuente de Datos
- **Videos de YouTube**: Canales especializados en trading y finanzas
- **Canales procesados**:
  - ARENA ALFA
  - Bitcoin hoy ÔΩú Esteban Perez Trader
  - Bolsas hoy ÔΩú Esteban P√©rez Inversor
  - Otros canales de trading

#### Caracter√≠sticas de los Datos
- **Formato**: Videos MP4 descargados de YouTube
- **Resoluci√≥n**: Variable (depende del video original)
- **Duraci√≥n**: Videos de diferentes longitudes
- **Contenido**: Frames con gr√°ficos financieros, tablas, texto, personas, logos, etc.

#### Exploraci√≥n de Datos (EDA)
El proyecto incluye an√°lisis exploratorio de datos que revela:
- Distribuci√≥n de videos por canal
- Estad√≠sticas de duraci√≥n de videos
- Distribuci√≥n temporal de publicaciones
- An√°lisis de frames extra√≠dos (histogramas, boxplots, etc.)

**Archivos de an√°lisis**:
- `src/notebooks/eda_videos_youtube.ipynb`: An√°lisis exploratorio de videos
- `src/notebooks/preprocessing_frames.ipynb`: An√°lisis de frames extra√≠dos

#### Desaf√≠os Identificados
1. **Redundancia de frames**: Muchos frames similares o duplicados
2. **Fondos uniformes**: Frames sin informaci√≥n relevante
3. **Variabilidad de contenido**: Diferentes tipos de elementos visuales
4. **Calidad variable**: Videos con diferentes resoluciones y calidades

---

### 3. Preparaci√≥n de los Datos

#### Pipeline de Preprocesamiento

##### 3.1 Descarga de Videos
- **Herramienta**: `yt-dlp` para descargar videos de YouTube
- **Almacenamiento**: Videos organizados por canal en `src/app/data/videos_youtube/`
- **Componente**: `ScrapperVideosYoutube`

##### 3.2 Extracci√≥n de Frames
- **Intervalo**: Extracci√≥n de frames cada 2 segundos
- **Formato**: Im√°genes PNG guardadas en `src/app/data/frames/`
- **Componente**: `ScrapperVideosYoutube.extract_frames_from_video()`

##### 3.3 Filtrado de Frames Duplicados
- **M√©todo**: SSIM (Structural Similarity Index)
- **Umbral**: 0.98 (frames con similitud > 98% se consideran duplicados)
- **Preservaci√≥n del orden temporal**: S√≠
- **Componente**: `FrameFilter`
- **Resultado**: Reducci√≥n t√≠pica del 30-50% de frames redundantes

##### 3.4 Filtrado de Fondos Uniformes
- **M√©todo**: An√°lisis de varianza de p√≠xeles
- **Objetivo**: Eliminar frames con fondos uniformes sin informaci√≥n relevante
- **Componente**: `Preprocessor`

##### 3.5 Extracci√≥n de Caracter√≠sticas (Embeddings)
- **Modelo**: ResNet-50 preentrenado en ImageNet
- **Dimensi√≥n de features**: 2048
- **Dispositivo**: CPU/GPU configurable
- **Batch size**: 32
- **Componente**: `FeatureExtractor`
- **Almacenamiento**: Embeddings guardados en `src/app/data/embeddings/`

#### Datasets Generados
- **Frames etiquetados**: Dataset de 10,000 muestras con etiquetas consensuadas entre CLIP y OpenAI GPT-4.1
- **Embeddings pregenerados**: Features extra√≠das con ResNet-50 (reutilizadas para clasificaci√≥n)
- **Ubicaci√≥n**: `src/notebooks/datasets/`

---

### 4. Modelado

#### 4.1 Extracci√≥n de Keyframes

##### M√©todo Principal: K-Means Clustering
Basado en el art√≠culo **"Static Video Summarization Using Transfer Learning and Clustering"** (Kashid et al.):

1. **Extracci√≥n de Features**: ResNet-50 preentrenado
2. **Clustering**: K-Means con distancia euclidiana
3. **Optimizaci√≥n**: Silhouette Score para determinar n√∫mero √≥ptimo de clusters
4. **Selecci√≥n de Keyframes**: Frame con mayor disimilitud al centroide de cada cluster

**Par√°metros**:
- M√©trica de distancia: Euclidiana o Coseno
- Normalizaci√≥n: Opcional (L2 normalization para cosine similarity)
- N√∫mero de clusters: Determinado autom√°ticamente usando Silhouette Score

**Componente**: `KeyFrameSelector`

**Notebooks relacionados**:
- `src/notebooks/articulo_clustering_video.ipynb`: Implementaci√≥n del m√©todo del art√≠culo
- `src/notebooks/articulo_clustering_video_sin_pca.ipynb`: Variante sin PCA

##### M√©todo Alternativo: Cosine Similarity
- Clustering basado en similitud coseno
- Normalizaci√≥n L2 de features
- √ötil para comparaci√≥n de caracter√≠sticas normalizadas

#### 4.2 Clasificaci√≥n de Frames

##### Modelo: AutoGluon TabularPredictor
- **Features de entrada**: Embeddings de ResNet-50 (2048 dimensiones)
- **Clases a predecir**: 11 categor√≠as
  - `background`: Fondos sin informaci√≥n relevante
  - `bar_chart`: Gr√°ficos de barras
  - `candlestick`: Gr√°ficos de velas (trading)
  - `diagram`: Diagramas y esquemas
  - `line_chart`: Gr√°ficos de l√≠neas
  - `logo`: Logos y marcas
  - `other`: Otros elementos
  - `person`: Personas en el video
  - `screenshot`: Capturas de pantalla
  - `table`: Tablas de datos
  - `text`: Texto visible

##### Proceso de Etiquetado del Dataset

El dataset de entrenamiento se cre√≥ mediante un proceso de etiquetado semi-autom√°tico que combina dos modelos para garantizar alta calidad:

1. **Selecci√≥n de Muestra**: Se seleccionaron 10,000 frames v√°lidos del dataset procesado, balanceados por canal y categor√≠a.

2. **Etiquetado con CLIP**:
   - Primera etapa de etiquetado usando el modelo CLIP (Contrastive Language-Image Pre-training)
   - Notebook: `frame_labeling_clip_v3.ipynb`
   - Clasificaci√≥n sem√°ntica basada en texto e imagen

3. **Etiquetado con OpenAI GPT-4.1**:
   - Segunda etapa usando OpenAI GPT-4.1 Vision API
   - Notebook: `frame_labeling_openai_v3.ipynb`
   - Se probaron varias versiones ajustando los prompts para optimizar la precisi√≥n
   - Revisi√≥n y selecci√≥n de los mejores prompts por categor√≠a

4. **Filtrado por Consenso**:
   - Solo se conservaron los frames donde **ambos modelos (CLIP y OpenAI) estaban de acuerdo** en la categor√≠a
   - Esto garantiza un dataset de alta calidad y confiabilidad
   - Resultado: Dataset final con etiquetas consensuadas entre ambos modelos

##### Entrenamiento del Clasificador

- **Dataset**: Frames etiquetados con consenso CLIP + OpenAI GPT-4.1
- **Tama√±o de entrenamiento**: 2,935 frames (frames con acuerdo entre modelos)
- **Tama√±o de prueba**: 734 frames
- **Tiempo de entrenamiento**: 30 minutos (time_limit=1800s)
- **Precisi√≥n alcanzada**: 94.14%

**Arquitectura del Modelo**:
- **Base**: ResNet-50 preentrenado (ya utilizado en la aplicaci√≥n para extracci√≥n de embeddings)
- **Cabeza de Clasificaci√≥n**: AutoGluon TabularPredictor entrenada sobre los embeddings de ResNet-50
- **Ventajas de esta arquitectura**:
  - **No consume API**: El modelo entrenado funciona offline sin necesidad de llamadas a OpenAI
  - **No requiere VIT multimodal**: Evita el uso de modelos Vision Transformer multimodales que tienen tiempos de inferencia altos
  - **Reutilizaci√≥n de embeddings**: Aprovecha los embeddings de ResNet-50 ya generados para la extracci√≥n de keyframes, optimizando el procesamiento

**Notebooks de entrenamiento**:
- `src/notebooks/clasificacion_frames.ipynb`: Entrenamiento con etiquetas consensuadas (CLIP + OpenAI)
- `src/notebooks/train_clip_classifier.ipynb`: Entrenamiento alternativo con etiquetas de CLIP √∫nicamente

**Modelos guardados**:
- `src/notebooks/models/classifier_resnet50_class/`: Modelo principal (entrenado con consenso CLIP + OpenAI)
- `src/notebooks/models/clip_classifier_resnet50_class/`: Modelo alternativo (entrenado solo con CLIP)

#### 4.3 Generaci√≥n de Descripciones Textuales

##### Modelo: OpenAI GPT-4o Vision API
- **Modelos disponibles**: `gpt-4o-mini`, `gpt-4o`, `gpt-4.1`
- **Prompts espec√≠ficos por categor√≠a**: Cada categor√≠a tiene un prompt optimizado
- **Componente**: `FrameDescriptionLlm`

**Prompts disponibles**:
- `background.txt`: Para fondos
- `person.txt`: Para personas
- `text.txt`: Para texto
- `screenshot.txt`: Para capturas de pantalla
- `diagram.txt`: Para diagramas
- `table.txt`: Para tablas
- `logo.txt`: Para logos
- `candlestick.txt`: Para gr√°ficos de velas
- `line_chart.txt`: Para gr√°ficos de l√≠neas
- `bar_chart.txt`: Para gr√°ficos de barras
- `other.txt`: Para otros elementos

**Ubicaci√≥n**: `src/app/prompts/`

---

### 5. Evaluaci√≥n

#### 5.1 M√©tricas de Clustering

##### Silhouette Score
- **Rango**: [-1, 1]
- **Interpretaci√≥n**: Valores cercanos a 1 indican clusters bien separados
- **Uso**: Optimizaci√≥n del n√∫mero de clusters

##### Reducci√≥n de Frames
- **M√©trica**: Porcentaje de reducci√≥n respecto a frames originales
- **Objetivo**: > 80% de reducci√≥n manteniendo informaci√≥n relevante

##### Distribuci√≥n de Clusters
- An√°lisis de tama√±o y distribuci√≥n de clusters
- Identificaci√≥n de clusters desbalanceados

#### 5.2 M√©tricas de Clasificaci√≥n

##### Precisi√≥n Global
- **Modelo ResNet-50 + Cabeza de Predicci√≥n**: 94.14%
- **Dataset de entrenamiento**: 2,935 frames (frames con acuerdo entre CLIP y OpenAI)
- **Dataset de prueba**: 734 frames
- **N√∫mero de clases**: 11 categor√≠as

##### Precisi√≥n por Clase
- An√°lisis de precisi√≥n, recall y F1-score por categor√≠a
- Identificaci√≥n de clases con menor rendimiento

##### Matriz de Confusi√≥n
- Visualizaci√≥n de errores de clasificaci√≥n
- Identificaci√≥n de confusiones entre clases similares

**Archivos de evaluaci√≥n**:
- `src/notebooks/clasificacion_frames.ipynb`: Evaluaci√≥n completa del clasificador
- `src/app/utils/evaluator.py`: M√≥dulo de evaluaci√≥n
- `src/notebooks/models/classifier_resnet50_class_info.json`: Informaci√≥n del modelo entrenado

#### 5.3 M√©tricas de Evaluaci√≥n de Etiquetado

El proceso de etiquetado del dataset se evalu√≥ mediante dos enfoques complementarios:

##### Evaluaci√≥n con LLM as Evaluator (GPT-4.1 calificando CLIP)

En el notebook `frame_labeling_openai_v3.ipynb`, se utiliz√≥ GPT-4.1 Vision API para evaluar las etiquetas generadas por CLIP, implementando un enfoque de "LLM as Evaluator":

- **Dataset evaluado**: 10,000 frames balanceados por canal y categor√≠a
- **Concordancia entre CLIP y OpenAI GPT-4.1**: 36.46% (3,646 / 10,000 frames)
- **Total de acuerdos**: 3,646 frames
- **Total de desacuerdos**: 6,354 frames
- **Dataset final usado para entrenamiento**: Solo los frames donde ambos modelos coincidieron (3,646 frames)

**Interpretaci√≥n**:
- La concordancia del 36.46% indica que ambos modelos tienen criterios diferentes pero complementarios
- El filtrado por consenso garantiza alta calidad: solo se usaron frames con acuerdo entre ambos modelos
- Este enfoque de validaci√≥n cruzada reduce errores de etiquetado y mejora la confiabilidad del dataset

**Notebook**: `src/notebooks/frame_labeling_openai_v3.ipynb`

##### M√©tricas de Confianza de CLIP

En el notebook `frame_labeling_clip_v3.ipynb`, se analizaron los scores de confianza del modelo CLIP:

- **Score promedio**: 0.2550
- **Score mediana**: 0.2546
- **Score m√≠nimo**: 0.1673
- **Score m√°ximo**: 0.3427
- **Desviaci√≥n est√°ndar**: 0.0179
- **Frames con baja confianza (score < 0.2)**: 449 frames (0.18% del total)

**Distribuci√≥n de scores por categor√≠a**:
- Las categor√≠as con mayor score promedio: `table` (0.259), `candlestick` (0.259), `bar_chart` (0.258)
- Las categor√≠as con menor score promedio: `logo` (0.226), `other` (0.234), `diagram` (0.237)

**Notebook**: `src/notebooks/frame_labeling_clip_v3.ipynb`

#### 5.4 M√©tricas de Cobertura Temporal
- **Cobertura porcentual**: Porcentaje del video cubierto por keyframes
- **Gaps temporales**: Intervalos sin keyframes seleccionados
- **Distribuci√≥n temporal**: An√°lisis de distribuci√≥n de keyframes a lo largo del video

#### 5.5 Visualizaciones Generadas
- Distribuci√≥n de categor√≠as
- Matrices de confusi√≥n
- Mosaicos de keyframes
- Gr√°ficos de precisi√≥n por clase
- An√°lisis de calidad de clasificaci√≥n

**Ubicaci√≥n de im√°genes**: `src/notebooks/images/`

---

### 6. Despliegue (Opcional)

#### Interfaz Web con Streamlit
El proyecto incluye una interfaz web interactiva para el procesamiento de videos:

**Componente**: `src/app/main_interface.py`

**Funcionalidades**:
1. **Cargar Videos**: Carga videos recientes de canales de YouTube
2. **Descargar y Extraer Frames**: Descarga videos y extrae frames
3. **Preprocesamiento**: Filtrado SSIM y generaci√≥n de embeddings
4. **Selecci√≥n de Keyframes**: Clustering y selecci√≥n de frames representativos
5. **Clasificaci√≥n**: Clasificaci√≥n autom√°tica de keyframes
6. **Descripci√≥n Textual**: Generaci√≥n de descripciones usando LLMs

**Caracter√≠sticas**:
- Sistema de cach√© para optimizar procesamiento
- Visualizaci√≥n de resultados en tiempo real
- Procesamiento por lotes
- Gesti√≥n de estado de sesi√≥n

**Ejecuci√≥n**:
```bash
streamlit run src/app/main_interface.py
```

---

## üõ†Ô∏è Instalaci√≥n y Configuraci√≥n

### Requisitos del Sistema
- Python 3.8+
- CUDA (opcional, para aceleraci√≥n GPU)

### Crear y Activar Entorno Virtual

**Opci√≥n 1: Usando venv (recomendado)**
```bash
# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
# En macOS/Linux:
source venv/bin/activate
# En Windows:
# venv\Scripts\activate
```

### Instalaci√≥n de Dependencias

```bash
pip install -r requirements.txt
```

### Dependencias Principales
- `torch>=2.0.0`: PyTorch para modelos de deep learning
- `torchvision>=0.15.0`: Modelos preentrenados (ResNet-50)
- `transformers>=4.30.0`: Modelos de transformers
- `openai>=1.0.0`: API de OpenAI para descripciones
- `autogluon.tabular`: Clasificador autom√°tico
- `streamlit`: Interfaz web
- `yt-dlp>=2025.9.26`: Descarga de videos de YouTube
- `opencv-python>=4.5.0`: Procesamiento de video
- `scikit-learn>=1.0.0`: Clustering y m√©tricas
- `scikit-image>=0.19.0`: SSIM para filtrado

### Configuraci√≥n de API Keys

Crear archivo `.env` en la ra√≠z del proyecto:

```env
OPENAI_API_KEY=tu_api_key_aqui
YOUTUBE_API_KEY=tu_youtube_api_key_aqui  # Opcional
```

---

## üìÅ Estructura del Proyecto

```
proyecto-vision-computador/
‚îú‚îÄ‚îÄ README.md                    # Este archivo
‚îú‚îÄ‚îÄ requirements.txt             # Dependencias del proyecto
‚îú‚îÄ‚îÄ .gitignore                   # Archivos ignorados por git
‚îÇ
‚îú‚îÄ‚îÄ articles/                    # Art√≠culos de referencia
‚îÇ   ‚îú‚îÄ‚îÄ articulo_clustering_video.pdf
‚îÇ   ‚îú‚îÄ‚îÄ kek_frame_extract_articulo.pdf
‚îÇ   ‚îî‚îÄ‚îÄ pdf_summary.txt
‚îÇ
‚îú‚îÄ‚îÄ images/                      # Im√°genes de an√°lisis y resultados
‚îÇ   ‚îú‚îÄ‚îÄ clusters_frames.png
‚îÇ   ‚îú‚îÄ‚îÄ conteo_canales.png
‚îÇ   ‚îú‚îÄ‚îÄ duracion_canales.png
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ app/                     # Aplicaci√≥n principal
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ main_interface.py    # Interfaz Streamlit
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îú‚îÄ‚îÄ data/                # Datos procesados
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cache/           # Cache de metadata
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings/     # Embeddings pregenerados
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frames/         # Frames extra√≠dos
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline_cache/ # Cache del pipeline
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ videos_youtube/ # Videos descargados
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îú‚îÄ‚îÄ llm/                 # Integraci√≥n con LLMs
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm.py          # FrameDescriptionLlm
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îú‚îÄ‚îÄ prompts/             # Prompts para LLMs
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ background.txt
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ person.txt
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text.txt
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îú‚îÄ‚îÄ utils/               # Utilidades del pipeline
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cache_manager.py      # Gesti√≥n de cach√©
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ categorizer.py        # Categorizaci√≥n global
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset_builder.py    # Construcci√≥n de datasets
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py          # Evaluaci√≥n de m√©tricas
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_extractor.py  # Extracci√≥n de features
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frame_filter.py       # Filtrado SSIM
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ keyframe_selector.py   # Selecci√≥n de keyframes
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocessor.py        # Preprocesamiento
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ results_comparator.py  # Comparaci√≥n de resultados
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îî‚îÄ‚îÄ youtube/             # Integraci√≥n con YouTube
    ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ       ‚îú‚îÄ‚îÄ scrapper_videos_youtube.py  # Descarga de videos
    ‚îÇ       ‚îî‚îÄ‚îÄ youtube_ingest.py           # Ingesta de metadata
    ‚îÇ   ‚îÇ
    ‚îî‚îÄ‚îÄ notebooks/               # Notebooks de an√°lisis y entrenamiento
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îú‚îÄ‚îÄ setup_path.py
        ‚îÇ
        ‚îú‚îÄ‚îÄ datasets/            # Datasets procesados
        ‚îÇ   ‚îú‚îÄ‚îÄ df_frames_openai_labeled_v3.csv
        ‚îÇ   ‚îú‚îÄ‚îÄ df_videos_frames_filtrados_v2.csv
        ‚îÇ   ‚îî‚îÄ‚îÄ ...
        ‚îÇ
        ‚îú‚îÄ‚îÄ models/              # Modelos entrenados
        ‚îÇ   ‚îú‚îÄ‚îÄ classifier_resnet50_class/
        ‚îÇ   ‚îú‚îÄ‚îÄ clip_classifier_resnet50_class/
        ‚îÇ   ‚îî‚îÄ‚îÄ *_class_info.json
        ‚îÇ
        ‚îú‚îÄ‚îÄ images/              # Visualizaciones generadas
        ‚îÇ   ‚îú‚îÄ‚îÄ accuracy_by_class_*.png
        ‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix_*.png
        ‚îÇ   ‚îî‚îÄ‚îÄ ...
        ‚îÇ
        ‚îú‚îÄ‚îÄ features_resnet50/   # Features pregeneradas
        ‚îÇ
        ‚îú‚îÄ‚îÄ frames/              # Frames de ejemplo
        ‚îÇ
        ‚îú‚îÄ‚îÄ videos_youtube/      # Videos de ejemplo
        ‚îÇ
        ‚îú‚îÄ‚îÄ eda_videos_youtube.ipynb              # EDA de videos
        ‚îú‚îÄ‚îÄ scrapper_videos_youtube.ipynb         # Scraping de videos
        ‚îú‚îÄ‚îÄ preprocessing_frames.ipynb            # Preprocesamiento
        ‚îú‚îÄ‚îÄ articulo_clustering_video.ipynb        # Clustering (art√≠culo)
        ‚îú‚îÄ‚îÄ articulo_clustering_video_sin_pca.ipynb
        ‚îú‚îÄ‚îÄ articulo_key_frames_cosine_similarity.ipynb  # Extracci√≥n de Keyframes (Cosine Similarity)
        ‚îú‚îÄ‚îÄ frame_labeling_openai_v3.ipynb         # Etiquetado OpenAI (varias versiones de prompts)
        ‚îú‚îÄ‚îÄ frame_labeling_clip_v3.ipynb          # Etiquetado CLIP (primera etapa)
        ‚îú‚îÄ‚îÄ clasificacion_frames.ipynb             # Clasificaci√≥n
        ‚îî‚îÄ‚îÄ train_clip_classifier.ipynb            # Entrenamiento CLIP
```

---

## üöÄ Uso del Proyecto

### Opci√≥n 1: Interfaz Web (Recomendado)

**Activar el entorno virtual primero:**
```bash
# Si usaste venv:
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate  # Windows

# Si usaste conda:
conda activate proyecto-vision
```

**Ejecutar la interfaz:**
```bash
streamlit run src/app/main_interface.py
```

La interfaz web permite:
1. Cargar videos de YouTube
2. Procesar videos paso a paso
3. Visualizar resultados en tiempo real
4. Generar descripciones textuales

### Opci√≥n 2: Uso Program√°tico

```python
from app.youtube.scrapper_videos_youtube import ScrapperVideosYoutube
from app.utils.frame_filter import FrameFilter
from app.utils.feature_extractor import FeatureExtractor
from app.utils.keyframe_selector import KeyFrameSelector

# 1. Descargar video y extraer frames
scrapper = ScrapperVideosYoutube(
    downloads_dir="data/videos_youtube",
    frames_dir="data/frames",
    frame_interval_sec=2
)
scrapper.download_videos_robust([video_url])
frames = scrapper.extract_frames_from_video(video_path, frames_dir)

# 2. Filtrar frames duplicados
frame_filter = FrameFilter(ssim_threshold=0.98)
filtered_frames, _, _ = frame_filter.filter_duplicate_frames(frames)

# 3. Extraer features
feature_extractor = FeatureExtractor(model_name='resnet50')
features, valid_paths = feature_extractor.extract_features_from_paths(filtered_frames)

# 4. Seleccionar keyframes
keyframe_selector = KeyFrameSelector(clustering_method='kmeans')
keyframes, labels, model, stats = keyframe_selector.select_keyframes(
    features, valid_paths, n_frames=len(valid_paths)
)
```

### Opci√≥n 3: Notebooks de An√°lisis

Ejecutar los notebooks en orden:
1. `eda_videos_youtube.ipynb`: Exploraci√≥n de datos
2. `preprocessing_frames.ipynb`: Preprocesamiento
3. `articulo_clustering_video.ipynb`: Extracci√≥n de keyframes
4. `clasificacion_frames.ipynb`: Clasificaci√≥n y evaluaci√≥n

---

## üìä Resultados

### Rendimiento del Clasificador

**Modelo**: ResNet-50 + AutoGluon TabularPredictor
- **Precisi√≥n global**: 94.14%
- **Dataset de entrenamiento**: 2,935 frames
- **Dataset de prueba**: 734 frames
- **Clases**: 11 categor√≠as

### Reducci√≥n de Frames

- **Filtrado SSIM**: 30-50% de reducci√≥n
- **Selecci√≥n de keyframes**: 80-90% de reducci√≥n total
- **Cobertura temporal**: > 70% del video cubierto

### Distribuci√≥n de Categor√≠as

Las categor√≠as m√°s comunes en videos de trading:
- `person`: Personas presentando
- `candlestick`: Gr√°ficos de velas
- `line_chart`: Gr√°ficos de l√≠neas
- `table`: Tablas de datos
- `text`: Texto visible

---

## üî¨ Estrategia de Evaluaci√≥n

### M√©tricas Utilizadas

1. **Clustering**:
   - Silhouette Score
   - Distribuci√≥n de clusters
   - Reducci√≥n de frames

2. **Clasificaci√≥n**:
   - Precisi√≥n global
   - Precisi√≥n por clase
   - Recall y F1-score
   - Matriz de confusi√≥n

3. **Cobertura Temporal**:
   - Porcentaje de cobertura
   - Gaps temporales
   - Distribuci√≥n de keyframes

### Estrategia de Validaci√≥n

- **Divisi√≥n train/test**: 80/20
- **Validaci√≥n cruzada**: Considerada para futuras mejoras
- **Evaluaci√≥n en m√∫ltiples videos**: Validaci√≥n en diferentes canales

---

## üéì Referencias y Art√≠culos

### Art√≠culos Implementados

1. **"Static Video Summarization Using Transfer Learning and Clustering"**
   - Autores: Shamal Kashid, Lalit K. Awasthi, Krishan Berwal, Parul Saini
   - M√©todo: K-Means clustering con ResNet-50
   - Implementaci√≥n: `articulo_clustering_video.ipynb`

2. **"Key-Frame Extraction Methods: A Review"**
   - M√©todos de extracci√≥n de keyframes
   - Implementaci√≥n: `articulo_key_frames_cosine_similarity.ipynb`

### Modelos Utilizados

- **ResNet-50**: Preentrenado en ImageNet
  - Extracci√≥n de embeddings para keyframes
  - Base para el clasificador de categor√≠as (reutilizaci√≥n de embeddings)
- **AutoGluon TabularPredictor**: Clasificador autom√°tico
  - Cabeza de clasificaci√≥n entrenada sobre embeddings de ResNet-50
- **OpenAI GPT-4.1 Vision**: Etiquetado del dataset y generaci√≥n de descripciones
  - Etiquetado: M√∫ltiples versiones de prompts ajustadas iterativamente
  - Generaci√≥n: Descripciones textuales de frames clasificados
- **CLIP**: Modelo de visi√≥n-lenguaje
  - Primera etapa de etiquetado del dataset
  - Validaci√≥n cruzada con OpenAI para garantizar calidad

---

## üîß Configuraci√≥n del Entorno de Entrenamiento

### Hardware Recomendado

- **CPU**: M√∫ltiples n√∫cleos (procesamiento paralelo)
- **RAM**: M√≠nimo 8GB, recomendado 16GB+
- **GPU**: Opcional pero recomendado para entrenamiento (CUDA compatible)
- **Almacenamiento**: Espacio suficiente para videos y frames (100GB+)

### Configuraci√≥n de Entrenamiento

- **Batch size**: 32 (ajustable seg√∫n memoria)
- **Device**: CPU o CUDA
- **Time limit**: 1800 segundos (30 minutos) para AutoGluon
- **Random state**: 42 (reproducibilidad)

---

## üìù Notas T√©cnicas

### Funci√≥n de Costo

- **Clustering**: Inercia (suma de distancias al cuadrado)
- **Clasificaci√≥n**: Cross-entropy (manejada por AutoGluon)
- **Optimizaci√≥n**: Minimizaci√≥n de inercia + maximizaci√≥n de Silhouette Score

### Aumento de Datos

- **No aplicado en este proyecto**: Dataset suficientemente grande
- **Posible mejora futura**: Data augmentation para clases desbalanceadas

### Adquisici√≥n de Datos Adicionales

- **Fuente principal**: YouTube (canales de trading)
- **Etiquetado**: Semi-autom√°tico mediante consenso entre CLIP y OpenAI GPT-4.1
  - Proceso iterativo con m√∫ltiples versiones de prompts
  - Selecci√≥n de 10,000 frames balanceados por canal y categor√≠a
  - Filtrado por acuerdo entre ambos modelos para garantizar calidad
- **Validaci√≥n**: Dataset final validado por consenso entre modelos (no requiere validaci√≥n manual extensiva)

---

## üéØ Conclusiones

### Extracci√≥n de Keyframes

**Cosine Similarity** ofrece una cobertura temporal y eficiencia muy superiores, procesando los videos 124 veces m√°s r√°pido y cubriendo 10 veces m√°s del contenido.

**K-Means** solo es ventajoso cuando se requiere la m√°xima compresi√≥n de datos, aunque sacrifica cobertura y velocidad.

**Cosine Similarity** es m√°s simple, preserva mejor la secuencia temporal y detecta cambios relevantes entre frames consecutivos.

Para la mayor√≠a de escenarios, **Cosine Similarity** es la opci√≥n recomendada; **K-Means** solo debe usarse si la compresi√≥n extrema es prioritaria y el tiempo de procesamiento no es cr√≠tico.

Implementar un sistema de extracci√≥n de informaci√≥n basado en descripciones de keyframes permite a un asistente de an√°lisis financiero para inversores individuales acceder de manera √°gil y eficiente a grandes vol√∫menes de videos de YouTube, sin sacrificar la cobertura informativa esencial.

En promedio, este m√©todo reduce el tiempo necesario para revisar el contenido en m√°s de un **55%**, asegurando que los usuarios puedan identificar r√°pidamente la informaci√≥n relevante para la toma de decisiones financieras, sin la necesidad de ver cada video completo.

---

### Clasificaci√≥n

El modelo **ResNet-50 + AutoGluon TabularPredictor** alcanza una precisi√≥n del **94.14%** en la clasificaci√≥n de frames en 11 categor√≠as, superando el objetivo inicial del 90%. Esta arquitectura h√≠brida aprovecha los embeddings de ResNet-50 ya generados para la extracci√≥n de keyframes, optimizando el procesamiento mediante la reutilizaci√≥n de caracter√≠sticas.

La estrategia de **reutilizaci√≥n de embeddings** elimina la necesidad de re-extraer caracter√≠sticas para la clasificaci√≥n, reduciendo significativamente el tiempo de procesamiento y el consumo de recursos computacionales. El modelo funciona completamente **offline** sin necesidad de llamadas a APIs externas, lo que garantiza privacidad, velocidad y reducci√≥n de costos operativos.

El clasificador demuestra un rendimiento excepcional en categor√≠as cr√≠ticas para el an√°lisis financiero: **person** (99.3%), **table** (96.4%), **candlestick** (93.3%), lo que valida su utilidad pr√°ctica para el dominio de aplicaci√≥n. La arquitectura modular permite actualizar el clasificador sin afectar el pipeline de extracci√≥n de keyframes, facilitando mejoras iterativas y mantenimiento del sistema.

---

### Etiquetado

El proceso de etiquetado semi-autom√°tico mediante **consenso entre CLIP y OpenAI GPT-4.1** demuestra ser una estrategia efectiva para crear datasets de alta calidad sin requerir validaci√≥n manual extensiva. La concordancia del **36.46%** entre ambos modelos, aunque aparentemente baja, garantiza que solo se conserven los frames con mayor confianza, resultando en un dataset de **3,646 frames** con etiquetas de alta calidad.

El enfoque de **"LLM as Evaluator"** implementado con GPT-4.1 calificando las etiquetas de CLIP permite una validaci√≥n cruzada automatizada que reduce significativamente los errores de etiquetado. La iteraci√≥n en m√∫ltiples versiones de prompts optimiza la precisi√≥n del etiquetado, demostrando que la ingenier√≠a de prompts es crucial para maximizar el rendimiento de los modelos de visi√≥n.

El balanceo del dataset por canal y categor√≠a asegura representatividad y reduce sesgos, mientras que el filtrado por consenso elimina frames ambiguos que podr√≠an degradar el rendimiento del clasificador. Este proceso semi-autom√°tico reduce el tiempo de etiquetado manual en m√°s del **90%** comparado con m√©todos tradicionales, manteniendo o mejorando la calidad del dataset.

---

### Preprocesamiento

El pipeline de preprocesamiento logra una **reducci√≥n acumulada del 97.53% en almacenamiento** (de 81 GB a 2.0 GB) mediante la conversi√≥n de im√°genes a embeddings, mientras mantiene la informaci√≥n esencial para el procesamiento posterior. El filtrado **SSIM con umbral 0.95** elimina el **52.16% de frames duplicados**, siendo la etapa m√°s efectiva de reducci√≥n antes de la extracci√≥n de keyframes.

El filtrado de fondos uniformes, aunque elimina solo el **0.16% de frames**, es crucial para eliminar contenido sin informaci√≥n relevante, mejorando la calidad del dataset y reduciendo el ruido en las etapas posteriores. La extracci√≥n de embeddings con **ResNet-50 preentrenado** en ImageNet proporciona caracter√≠sticas robustas y generalizables que son efectivas tanto para clustering como para clasificaci√≥n.

El procesamiento por lotes (batch_size=32) optimiza el uso de recursos computacionales, especialmente cuando se utiliza GPU, reduciendo el tiempo de extracci√≥n de features de manera significativa. La preservaci√≥n del orden temporal durante el filtrado SSIM es esencial para mantener la coherencia narrativa del video, permitiendo que los keyframes seleccionados representen adecuadamente la secuencia temporal del contenido.

---

### Despliegue

La interfaz web con **Streamlit** proporciona una soluci√≥n accesible y f√°cil de usar para el procesamiento de videos, permitiendo a usuarios no t√©cnicos aprovechar el sistema completo sin necesidad de conocimientos de programaci√≥n. El sistema de **cach√© inteligente** implementado reduce dr√°sticamente los tiempos de procesamiento en ejecuciones repetidas, almacenando resultados intermedios (frames filtrados, embeddings, keyframes, clasificaciones) y evitando reprocesamiento innecesario.


La integraci√≥n con APIs externas (OpenAI GPT-4.1) para generaci√≥n de descripciones es opcional y se ejecuta solo cuando se requiere, manteniendo el sistema funcional incluso sin conexi√≥n a servicios externos. Esta flexibilidad hace que el sistema sea robusto y adaptable a diferentes entornos de despliegue, desde desarrollo local hasta producci√≥n en la nube.
